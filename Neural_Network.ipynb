{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ_rWY7rpWxr",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network implementation (using only numpy)\n",
        "\n",
        "## 1. Implement Activation Functions\n",
        "> sigmoid, tanh, relu, leaky_relu\n",
        "\n",
        "## 2. Design a class Neural Network\n",
        "> constructor, addHiddenLayer, parameter initializer, forward propagation, backward propagation, predict\n",
        "\n",
        "## 3. Implement Constructor\n",
        "\n",
        "## 4. Implement ```addHiddenLayer```, ```initialize_parameters```,  ```compute_cost```\n",
        "\n",
        "## 5. Implement ```forward_propagation```, ```backward_propagation```,  ```update_parameters```\n",
        "\n",
        "## 6. Implement ```run_model```, ```predict```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hC8b_0LzKJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pakage Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRVeN8j-zYHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Activation Functions\n",
        "def sigmoid(z, derivative = False):\n",
        "    \"\"\"Sigmoid Function\"\"\"\n",
        "    \n",
        "    if derivative:\n",
        "      a = sigmoid(z)\n",
        "      return a * (1 - a)\n",
        "    \n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def tanh(z, derivative = False):\n",
        "    \"\"\"Hyperbolic Tangent Function\"\"\"\n",
        "    \n",
        "    if derivative:\n",
        "      a = tanh(z)\n",
        "      return 1 - np.power(a, 2)\n",
        "    \n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z, derivative = False):\n",
        "    \"\"\"Rectified Linear Unit Function\"\"\"\n",
        "    \n",
        "    if derivative:\n",
        "        return z > 0\n",
        "    \n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def leaky_relu(z, scale = 0.01, derivative = False):\n",
        "    \"\"\"Leaky ReLU\"\"\"\n",
        "    \n",
        "    if derivative:\n",
        "        return z > 0 + (z <= 0) * scale\n",
        "      \n",
        "    return np.maximum(scale * z, z)\n",
        "\n",
        "class NeuralNetwork:    \n",
        "    def __init__(self, activation_function = sigmoid):\n",
        "        self.n = [-1, 1]\n",
        "        self.actFunc = [0, activation_function]\n",
        "    \n",
        "    def addHiddenLayer(self, number_of_units:int, activation_function = relu):\n",
        "        self.n.insert(len(self.n) - 1, number_of_units)\n",
        "        self.actFunc.insert(len(self.actFunc) - 1, activation_function)\n",
        "\n",
        "    def initialize_parameters(self, X, scale = 0.01):\n",
        "        print('# of Layers =', len(self.n))\n",
        "        number_of_layers = len(self.n)\n",
        "        self.W = [0] * number_of_layers\n",
        "        self.dW = [0] * number_of_layers\n",
        "        self.b = [0] * number_of_layers\n",
        "        self.db = [0] * number_of_layers\n",
        "        self.A = [0] * number_of_layers\n",
        "        self.Z = [0] * number_of_layers\n",
        "        self.dZ = [0] * number_of_layers\n",
        "        \n",
        "        self.A[0] = X\n",
        "        self.n[0] = X.shape[0]\n",
        "        \n",
        "        for i in range(1, len(self.n)):\n",
        "            self.W[i] = np.random.randn(self.n[i], self.n[i - 1]) * scale\n",
        "            self.b[i] = np.zeros((self.n[i], 1))\n",
        "            \n",
        "    def compute_cost(self, Y):\n",
        "        Y_hat = self.A[-1]\n",
        "        m = Y.shape[1] # number of example\n",
        "\n",
        "        # Compute the cross-entropy cost\n",
        "        logprobs = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
        "        cost = - np.sum(logprobs) / m\n",
        "\n",
        "        cost = np.squeeze(cost)\n",
        "        assert(isinstance(cost, float))\n",
        "\n",
        "        return cost\n",
        "      \n",
        "    def forward_propagation(self):\n",
        "        for i in range(1, len(self.n)):\n",
        "            self.Z[i] = self.W[i] @ self.A[i - 1] + self.b[i]\n",
        "            self.A[i] = self.actFunc[i](self.Z[i])\n",
        "            \n",
        "    def backward_propagation(self, Y):\n",
        "        \"\"\"\n",
        "          Implement the backward propagation using the instructions above.\n",
        "\n",
        "          Arguments:\n",
        "          X -- input data of shape (2, number of examples)\n",
        "          Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "          Returns:\n",
        "          grads -- python dictionary containing your gradients with respect to different parameters\n",
        "        \"\"\"\n",
        "        \n",
        "        m = self.A[0].shape[1]\n",
        "\n",
        "        # Backward propagation: calculate dW, db\n",
        "        \n",
        "        self.dZ[-1] = self.A[-1] - Y\n",
        "        self.dW[-1] = 1 / m * self.dZ[-1] @ self.A[-2].T\n",
        "        self.db[-1] = 1 / m * np.sum(self.dZ[-1], axis = 1, keepdims = True)\n",
        "        \n",
        "        for i in range(len(self.A) - 2, 0, -1):\n",
        "            self.dZ[i] = self.W[i + 1].T @ self.dZ[i + 1] * self.actFunc[i](self.A[i], derivative = True)\n",
        "            self.dW[i] = 1 / m * self.dZ[i] @ self.A[i-1].T\n",
        "            self.db[i] = 1 / m * np.sum(self.dZ[i], axis = 1, keepdims = True)\n",
        "\n",
        "    def update_parameters(self, learning_rate):\n",
        "        \"\"\"\n",
        "          Updates parameters using the gradient descent update rule\n",
        "        \"\"\"\n",
        "        \n",
        "        # Update rule for each parameter\n",
        "        number_of_layers = len(self.n) - 1\n",
        "        for i in range(1, number_of_layers + 1):\n",
        "            self.W[i] = self.W[i] - learning_rate * self.dW[i]\n",
        "            self.b[i] = self.b[i] - learning_rate * self.db[i]\n",
        "    \n",
        "    def run_model(self, X, Y, learning_rate = 0.01, num_iterations = 10000, print_cost = False):\n",
        "        \"\"\"\n",
        "            Arguments:\n",
        "            X -- dataset of shape (2, number of examples)\n",
        "            Y -- labels of shape (1, number of examples)\n",
        "            learning_rate -- Learning Rate of Gradient Descent\n",
        "            num_iterations -- Number of iterations in gradient descent loop\n",
        "            print_cost -- if True, print the cost every 1000 iterations\n",
        "        \"\"\"\n",
        "        \n",
        "        # Initialize parameters.\n",
        "        self.initialize_parameters(X)        \n",
        "\n",
        "        # Loop (gradient descent)\n",
        "        for i in range(num_iterations):\n",
        "\n",
        "            # Forward propagation.\n",
        "            self.forward_propagation()\n",
        "\n",
        "            # Cost function.\n",
        "            cost = self.compute_cost(Y)\n",
        "\n",
        "            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "            self.backward_propagation(Y)\n",
        "\n",
        "            # Gradient descent parameter update.\n",
        "            self.update_parameters(learning_rate)\n",
        "\n",
        "            # Print the cost every 1000 iterations\n",
        "            if print_cost and i % 1000 == 0:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "                \n",
        "    def predict(self, X, Y, threshold = 0.5):\n",
        "        \"\"\"\n",
        "        Using the learned parameters, predicts a class for each example in X\n",
        "\n",
        "        Arguments:\n",
        "        X -- input data of size (n_x, m)\n",
        "        Y -- output data of size (1, m)\n",
        "\n",
        "        Returns\n",
        "        predictions -- vector of predictions of our model\n",
        "        \"\"\"\n",
        "        \n",
        "        A = [0] * len(self.n)\n",
        "        Z = [0] * len(self.n)\n",
        "        A[0] = X\n",
        "\n",
        "        # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "        for i in range(1, len(self.n)):\n",
        "            Z[i] = self.W[i] @ A[i - 1] + self.b[i]\n",
        "            A[i] = self.actFunc[i](Z[i])\n",
        "            \n",
        "        predictions = A[-1] > threshold\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji4Xaa8crHK-",
        "colab_type": "text"
      },
      "source": [
        "# Make a new neural network with two hidden layers\n",
        "\n",
        "> ## First hidden layer has 5 hidden units.\n",
        "\n",
        "> ## Second hidden layer has 4 hidden units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae3t344s786a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = NeuralNetwork()\n",
        "network.addHiddenLayer(5)\n",
        "network.addHiddenLayer(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM_Mx63erWh_",
        "colab_type": "text"
      },
      "source": [
        "# Make a training set and a test set with random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj42mTT18D-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_size, test_set_size = 10000, 1000\n",
        "X = np.random.randn(5, training_set_size); Y = np.sum(X, axis = 0, keepdims = True) > 0; Y = Y.reshape(1, -1)\n",
        "test_X = np.random.randn(5, test_set_size); test_Y = np.sum(test_X, axis = 0, keepdims = True) > 0; test_Y = test_Y.reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAYj3ZrFrjzA",
        "colab_type": "text"
      },
      "source": [
        "# Run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJuE3xKTNNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "87ba8664-407f-43d7-a34b-1f890af356a9"
      },
      "source": [
        "network.run_model(X, Y, learning_rate = 0.0355, print_cost = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of Layers = 4\n",
            "Cost after iteration 0: 0.693149\n",
            "Cost after iteration 1000: 0.693134\n",
            "Cost after iteration 2000: 0.693134\n",
            "Cost after iteration 3000: 0.693132\n",
            "Cost after iteration 4000: 0.693129\n",
            "Cost after iteration 5000: 0.693121\n",
            "Cost after iteration 6000: 0.693077\n",
            "Cost after iteration 7000: 0.692467\n",
            "Cost after iteration 8000: 0.176914\n",
            "Cost after iteration 9000: 0.040253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd2tH273rnqS",
        "colab_type": "text"
      },
      "source": [
        "# Predict the results by a trained neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXJc2vaSXv1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = network.predict(test_X, test_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpk90lQsroeD",
        "colab_type": "text"
      },
      "source": [
        "# Print the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENbn_cSSkTvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab1844ed-aaf1-461a-d0a5-8ce1889ecd26"
      },
      "source": [
        "print('Accuracy: %d%%' % (float(test_Y @ predictions.T + (1 - test_Y) @ (1 - predictions.T)) / float(test_Y.size) * 100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 50%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}